# ========================================================
# CONFIGURACIÓN DE SPARK - ESTÁNDAR DE PROYECTO
# ========================================================
spark.app.name: "Lomar_ModernStack"
spark.master: "local[*]"

# --- GESTIÓN DE RECURSOS ---
spark.driver.memory: "4g"

# --- ESTÁNDARES DE DATOS ---
spark.sql.session.timeZone: "UTC"
spark.sql.sources.partitionOverwriteMode: "dynamic"

# --- OPTIMIZACIÓN DE RENDIMIENTO ---
spark.sql.shuffle.partitions: 8
spark.sql.adaptive.enabled: true
spark.sql.adaptive.advisoryPartitionSizeInBytes: 134217728

# --- CONECTOR GCS (Google Cloud Storage) ---
spark.hadoop.fs.gs.impl: "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem"
spark.hadoop.fs.AbstractFileSystem.gs.impl: "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS"

# --- RUTAS DE DEPENDENCIAS (Relativas a la raíz del proyecto) ---
# Nota: El Hook de Python se encarga de convertir esto en rutas absolutas 
# para que Spark no se pierda en WSL o Docker.
spark.jars: "gcs-connector-hadoop3-latest.jar,mssql-jdbc-12.8.1.jre11.jar"